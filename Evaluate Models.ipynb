{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models\n",
    "\n",
    "This notebook is used to load the trained models and check their performance on a test dataset which was generated in the \"Data Split\" Notebook.\n",
    "\n",
    "The models have been trained using the python scripts and submitted to the cluster with spark-submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SQLContext, SparkConf\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.functions import  when, col, rand, isnan, split, array,udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.recommendation import ALSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"RecSys-Challenge-Evaluate-Model\").setMaster(\"yarn\")\n",
    "conf = (conf.set(\"deploy-mode\",\"cluster\")\n",
    "       .set(\"spark.driver.memory\",\"100g\")\n",
    "       .set(\"spark.executor.memory\",\"100g\")\n",
    "       .set(\"spark.driver.cores\",\"1\")\n",
    "       .set(\"spark.num.executors\",\"100\")\n",
    "       .set(\"spark.executor.cores\",\"4\")\n",
    "       .set(\"spark.driver.maxResultSize\", \"100g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"hdfs:///user/e1553958/RSC20/test.tsv\"\n",
    "\n",
    "df = (sql.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"sep\", \"\\x01\")\n",
    "    .load(datafile,  inferSchema=\"true\")\n",
    "    .repartition(500)\n",
    "    .toDF(\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\"engaging_user_account_creation\", \"engaged_follows_engaging\", \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"engaged_with_user_is_verified\",col(\"engaged_with_user_is_verified\").cast(\"Integer\"))\n",
    "df = df.withColumn(\"engaging_user_is_verified\",col(\"engaging_user_is_verified\").cast(\"Integer\"))\n",
    "df = df.withColumn(\"engaged_follows_engaging\",col(\"engaged_follows_engaging\").cast(\"Integer\"))\n",
    "\n",
    "# Split the string representations of lists\n",
    "## Convert the text tokens to array of ints\n",
    "split_text = pyspark.sql.functions.split(df['text_tokens'], '\\t')\n",
    "df = df.withColumn(\"text_tokens\", split_text)\n",
    "\n",
    "## Convert present media to array of strings\n",
    "split_text = pyspark.sql.functions.split(df['present_media'], '\\t')\n",
    "df = df.withColumn(\"present_media\", when(col('present_media').isNull(), array().cast(\"array<string>\")).otherwise(split_text))\n",
    "\n",
    "## Convert present links to array of strings\n",
    "split_text = pyspark.sql.functions.split(df['present_links'], '\\t')\n",
    "df = df.withColumn(\"present_links\", when(col('present_links').isNull(), array().cast(\"array<string>\")).otherwise(split_text))\n",
    "\n",
    "## Convert hashtags to array of strings\n",
    "split_text = pyspark.sql.functions.split(df['hashtags'], '\\t')\n",
    "df = df.withColumn(\"hashtags\", when(col('hashtags').isNull(), array().cast(\"array<string>\")).otherwise(split_text))\n",
    "\n",
    "## Convert present_domains to array of strings\n",
    "split_text = pyspark.sql.functions.split(df['present_domains'], '\\t')\n",
    "df = df.withColumn(\"present_domains\", when(col('present_domains').isNull(), array().cast(\"array<string>\")).otherwise(split_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_response(x):\n",
    "    '''\n",
    "    Encode a response columnm with 0 or 1\n",
    "    Parameter\n",
    "    ---------\n",
    "    x: str\n",
    "        Name of the column to encode\n",
    "    Return\n",
    "        Int: 0 if no response, 1 if response\n",
    "    '''\n",
    "    return when(col(x).isNull(), float(0)).otherwise(float(1))\n",
    "\n",
    "get_probability=udf(lambda v:float(v[1]),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['reply_timestamp', \n",
    "                    'retweet_timestamp',\n",
    "                    'retweet_with_comment_timestamp', \n",
    "                    'like_timestamp'\n",
    "                    ]\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Random Forest without Tweet Features\n",
    "\n",
    "Model trained with \"Baseline_Approach_rf_train.py\" and stored on the HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = PipelineModel.load(\"hdfs:///user/e1553958/RecSys/datasplit/pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = rf_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For reply_timestamp: Area under PR = 0.030799386073847527\n",
      "For retweet_timestamp: Area under PR = 0.10276490043817094\n",
      "For retweet_with_comment_timestamp: Area under PR = 0.007482488732274796\n",
      "For like_timestamp: Area under PR = 0.5479100593045614\n"
     ]
    }
   ],
   "source": [
    "metrics[\"Random Forest\"] = {}\n",
    "\n",
    "for target_col in target_cols:\n",
    "    rf_data = rf_data.withColumn(target_col+\"_proba\", get_probability(target_col+\"_proba\"))\n",
    "    rf_data = rf_data.withColumn(target_col, encode_response(target_col))\n",
    "    \n",
    "    predictionAndLabels = rf_data.rdd.map(lambda r: (r[target_col+\"_proba\"], r[target_col]))\n",
    "    metric = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    \n",
    "    metrics[\"Random Forest\"][target_col] = metric.areaUnderPR\n",
    "    print(\"For {}: Area under PR = {}\".format(target_col, metrics[\"Random Forest\"][target_col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression without Tweet Features\n",
    "\n",
    "Model trained with \"Baseline_Approach_logReg_train.py\" and stored on the HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg_model = PipelineModel.load(\"hdfs:///user/e1553958/RecSys/datasplit/pipeline_logReg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg_data = logReg_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For reply_timestamp: Area under PR = 0.055998903236944404\n",
      "For retweet_timestamp: Area under PR = 0.14223024178532603\n",
      "For retweet_with_comment_timestamp: Area under PR = 0.012504503368450139\n",
      "For like_timestamp: Area under PR = 0.5455455147453278\n"
     ]
    }
   ],
   "source": [
    "metrics[\"Logistic Regression\"] = {}\n",
    "\n",
    "for target_col in target_cols:\n",
    "    logReg_data = logReg_data.withColumn(target_col+\"_proba\", get_probability(target_col+\"_proba\"))\n",
    "    logReg_data = logReg_data.withColumn(target_col, encode_response(target_col))\n",
    "    \n",
    "    predictionAndLabels = logReg_data.rdd.map(lambda r: (r[target_col+\"_proba\"], r[target_col]))\n",
    "    metric = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    \n",
    "    metrics[\"Logistic Regression\"][target_col] = metric.areaUnderPR\n",
    "    print(\"For {}: Area under PR = {}\".format(target_col, metrics[\"Logistic Regression\"][target_col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Matrix Factorization\n",
    "\n",
    "Model trained with \"Matrix_Factorization_train.py\" and stored on the HDFS. The fallback prediction is only a random generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(old, new):\n",
    "        \"\"\"\n",
    "        check if ID from training exits\n",
    "        if yes then use this id\n",
    "        if not use newly generated id\n",
    "        \"\"\"\n",
    "        if old == \"user\":\n",
    "            max_val = max_user_id\n",
    "        elif old == \"tweet\":\n",
    "            max_val = max_tweet_id\n",
    "        return when(col(old).isNull(), col(new) + max_val).otherwise(col(old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings_path = \"hdfs:///user/e1553958/RecSys/datasplit/mappings/\"\n",
    "# Load id_string to id mappings from training\n",
    "user2id = sql.read.format('parquet').load(mappings_path+\"user2id\")\n",
    "tweet2id = sql.read.format('parquet').load(mappings_path+\"tweet2id\")\n",
    "\n",
    "# Select relevant columns\n",
    "val_df = df.select(\"tweet_id\",\"engaging_user_id\", 'reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp')\n",
    "# Create mapping from id_string to id\n",
    "tweet2id_val = val_df.select(\"tweet_id\").rdd.map(lambda x: x[0]).distinct().zipWithUniqueId()\n",
    "user2id_val = val_df.select(\"engaging_user_id\").rdd.map(lambda x: x[0]).distinct().zipWithUniqueId()\n",
    "tweet2id_val = tweet2id_val.toDF().withColumnRenamed(\"_1\", \"tweet_id_str_val\").withColumnRenamed(\"_2\", \"tweet_new\")\n",
    "user2id_val = user2id_val.toDF().withColumnRenamed(\"_1\", \"user_id_str_val\").withColumnRenamed(\"_2\", \"user_new\")\n",
    "# Join Mapping with Dataframe\n",
    "val_df = val_df.join(tweet2id_val, col(\"tweet_id\") == col(\"tweet_id_str_val\"), \"left_outer\")\n",
    "val_df = val_df.join(user2id_val, col(\"engaging_user_id\") == col(\"user_id_str_val\"), \"left_outer\")\n",
    "# Join Mapping from training data with Dataframe\n",
    "val_df = val_df.join(tweet2id, col(\"tweet_id\") == col(\"tweet_id_str\"), \"left_outer\")\n",
    "val_df = val_df.join(user2id, col(\"engaging_user_id\") == col(\"user_id_str\"), \"left_outer\")\n",
    "\n",
    "# Get the maximum IDs from training\n",
    "max_user_id = user2id.groupBy().max(\"user\").collect()[0][0]\n",
    "max_tweet_id = tweet2id.groupBy().max(\"tweet\").collect()[0][0]\n",
    "\n",
    "\n",
    "val_df = val_df.withColumn(\"user\", create_index(\"user\", \"user_new\"))\n",
    "val_df = val_df.withColumn(\"tweet\", create_index(\"tweet\", \"tweet_new\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fallback_prediction(x):\n",
    "    \"\"\"\n",
    "    Make a random Guess if model made no predicitons\n",
    "    \"\"\"\n",
    "    return when(isnan(x), rand()).otherwise(col(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For reply_timestamp: Area under PR = 0.025947249265459268\n",
      "For retweet_timestamp: Area under PR = 0.09975892602345017\n",
      "For retweet_with_comment_timestamp: Area under PR = 0.007482329907789703\n",
      "For like_timestamp: Area under PR = 0.3986788910896872\n"
     ]
    }
   ],
   "source": [
    "metrics[\"Matrix Factorization\"] = {}\n",
    "\n",
    "for target_col in target_cols:\n",
    "        # Load model\n",
    "        model = ALSModel.load(\"hdfs:///user/e1553958/RecSys/datasplit/models/\" + target_col[:-10] + \"_als_model\")\n",
    "        # Get Predictions of the model\n",
    "        result_df = model.transform(val_df)\n",
    "        result_df = result_df.withColumn(\"prediction\", fallback_prediction(\"prediction\"))\n",
    "        result_df = result_df.withColumn(target_col, encode_response(target_col))\n",
    "        predictionAndLabels = result_df.rdd.map(lambda r: (r[\"prediction\"], r[target_col]))\n",
    "        metric = BinaryClassificationMetrics(predictionAndLabels)\n",
    "        metrics[\"Matrix Factorization\"][target_col] = metric.areaUnderPR\n",
    "        print(\"For {}: Area under PR = {}\".format(target_col, metrics[\"Matrix Factorization\"][target_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
