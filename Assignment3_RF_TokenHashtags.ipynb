{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext, SQLContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType, FloatType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, CountVectorizer, IDF, ChiSqSelector, Normalizer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Session for Assignment 3 - Pipeline (Token IDs and Hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Spark Config\n",
    "conf = SparkConf().setAppName(\"RecSys-Challenge-Train-Model\").setMaster(\"yarn\")\n",
    "conf = (conf.set(\"deploy-mode\",\"cluster\")\n",
    "       .set(\"spark.driver.memory\",\"100g\")\n",
    "       .set(\"spark.executor.memory\",\"100g\")\n",
    "       .set(\"spark.driver.cores\",\"1\")\n",
    "       .set(\"spark.num.executors\",\"50\")\n",
    "       .set(\"spark.executor.cores\",\"5\")\n",
    "       .set(\"spark.driver.maxResultSize\", \"100g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trainset of RecSys Challenge 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'hdfs:///user/pknees/RSC20/training.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (sql.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"sep\", \"\\x01\")\n",
    "    .load(path,  inferSchema=\"true\")\n",
    "    .repartition(1000)\n",
    "    .toDF(\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\"engaging_user_account_creation\", \"engaged_follows_engaging\", \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(['text_tokens', 'hashtags', 'reply_timestamp', \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+-----------------+------------------------------+--------------+\n",
      "|         text_tokens|            hashtags|reply_timestamp|retweet_timestamp|retweet_with_comment_timestamp|like_timestamp|\n",
      "+--------------------+--------------------+---------------+-----------------+------------------------------+--------------+\n",
      "|101\t49872\t11397\t1...|                null|           null|             null|                          null|          null|\n",
      "|101\t10159\t16304\t5...|                null|           null|             null|                          null|          null|\n",
      "|101\t56898\t137\t122...|                null|           null|             null|                          null|          null|\n",
      "|101\t12360\t10233\t1...|                null|           null|             null|                          null|    1580971528|\n",
      "|101\t37025\t117\t119...|9E05D7B39B177CC81...|           null|       1581387586|                          null|    1581387586|\n",
      "|101\t56898\t137\t278...|                null|           null|             null|                          null|          null|\n",
      "|101\t26543\t97938\t1...|                null|           null|             null|                          null|          null|\n",
      "|101\t56898\t137\t373...|                null|           null|       1581366587|                          null|    1581366587|\n",
      "|101\t56898\t137\t179...|                null|           null|             null|                          null|    1581333716|\n",
      "|101\t75877\t78254\t1...|                null|           null|             null|                          null|    1581176550|\n",
      "|101\t20843\t10712\t2...|                null|           null|             null|                          null|          null|\n",
      "|101\t56898\t137\t723...|                null|           null|             null|                          null|          null|\n",
      "|101\t39774\t41275\t1...|                null|           null|             null|                          null|    1581003383|\n",
      "|101\t100\t766\t10556...|                null|           null|             null|                          null|    1581368291|\n",
      "|101\t772\t23112\t619...|                null|           null|             null|                          null|          null|\n",
      "|101\t13497\t23964\t1...|                null|           null|             null|                          null|    1581413687|\n",
      "|101\t1936\t18628\t18...|                null|           null|             null|                          null|          null|\n",
      "|101\t56898\t137\t151...|                null|           null|             null|                          null|          null|\n",
      "|101\t69342\t11517\t5...|                null|           null|             null|                          null|          null|\n",
      "|101\t53798\t41588\t1...|                null|           null|             null|                          null|    1581440257|\n",
      "+--------------------+--------------------+---------------+-----------------+------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.limit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Engagements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_cols = ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in response_cols:\n",
    "    df = df.withColumn(\n",
    "        col,\n",
    "        F.when((F.col(col) >= 0), 1)\\\n",
    "        .otherwise(0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Token ID's with Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({'hashtags':'nohashtag'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    'token_and_hashtags',\n",
    "    F.concat(F.col(\"text_tokens\"), F.lit(\"\\t\"), F.col(\"hashtags\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = df.randomSplit([0.8, 0.2], 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline to get TF-IDF, Top Features and Modeltuning from Token ID's & Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordList = [\"101\",\"102\"] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "stopwordList = list(set(stopwordList))#optionnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of four stages: tokenizer, remover, CountVectorizer and idf.\n",
    "\n",
    "# RegEx Tokenizer which is dealing with our needed patterns [^a-zA-Z0-9] and lowercases all tokens.\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"token_and_hashtags\", outputCol=\"words\", pattern=\"[^a-zA-Z0-9]\")\n",
    "\n",
    "# Removes the default english stopwords can be checked with:  StopWordsRemover.loadDefaultStopWords('english')\n",
    "remover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(), outputCol=\"filtered\", stopWords=stopwordList)\n",
    "\n",
    "# The hash function used here is MurmurHash 3. Then term frequencies are calculated based on the mapped indices.\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "\n",
    "# Calculating TFIDF\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Chi Square Selector which selects the top 4000 features.\n",
    "selector = ChiSqSelector(featuresCol=idf.getOutputCol(),\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "\n",
    "# Normalize\n",
    "normalizer = Normalizer(p=2.0, inputCol=\"selectedFeatures\", outputCol=\"normalizedFeatures\")\n",
    "\n",
    "# Random Forest.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=normalizer.getOutputCol(), seed=1234)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, remover, cv, idf, selector, normalizer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(selector.numTopFeatures, [2000, 4000])\\\n",
    "    .addGrid(rf.numTrees, [100, 300, 500]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10])\\\n",
    "    .addGrid(rf.minInfoGain, [0.0, 0.1])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(selector.numTopFeatures, [4000])\\\n",
    "    .addGrid(rf.numTrees, [100, 300, 500]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10])\\\n",
    "    .addGrid(rf.minInfoGain, [0.1])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "                           # 75% of the data will be used for training, 25% for validation.\n",
    "                           seed=123,\n",
    "                           trainRatio=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 1st Model \"reply_timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = train_val.withColumnRenamed(\"reply_timestamp\", \"label\")\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model_reply = tvs.fit(train_val)\n",
    "train_val = train_val.withColumnRenamed(\"label\", \"reply_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model from cv grid search\n",
    "mPath_reply =  \"model_reply_bestModel_big\"\n",
    "model_reply.bestModel.write().overwrite().save(mPath_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2nd Model \"retweet_timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = train_val.withColumnRenamed(\"retweet_timestamp\", \"label\")\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model_retweet = tvs.fit(train_val)\n",
    "train_val = train_val.withColumnRenamed(\"label\", \"retweet_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model from cv grid search\n",
    "mPath_retweet =  \"model_retweet_bestModel_big\"\n",
    "model_retweet.bestModel.write().overwrite().save(mPath_retweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 3rd Model \"retweet_with_comment_timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = train_val.withColumnRenamed(\"retweet_with_comment_timestamp\", \"label\")\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model_retweet_with_comment = tvs.fit(train_val)\n",
    "train_val = train_val.withColumnRenamed(\"label\", \"retweet_with_comment_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model from cv grid search\n",
    "mPath_retweet_with_comment =  \"model_retweet_with_comment_bestModel_big\"\n",
    "model_retweet_with_comment.bestModel.write().overwrite().save(mPath_retweet_with_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 4th Model \"like_timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = train_val.withColumnRenamed(\"like_timestamp\", \"label\")\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model_like = tvs.fit(train_val)\n",
    "train_val = train_val.withColumnRenamed(\"label\", \"like_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model from cv grid search\n",
    "mPath_like =  \"model_like_bestModel_big\"\n",
    "model_like.bestModel.write().overwrite().save(mPath_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistedModel = PipelineModel.load(mPath_like)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
