{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Approach\n",
    "\n",
    "This Notebook tries to implement the baseline approach described in the paper accompanind the Exam: \n",
    "\n",
    "- [Privacy-Preserving Recommender Systems Challenge on Twitterâ€™s HomeTimeline]{https://arxiv.org/pdf/2004.13715.pdf}\n",
    "\n",
    "The baseline approch is described in chapter 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/spark-2.4.1\")\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.SparkContext.setSystemProperty('spark.executor.memory', '14g')\n",
    "sc = pyspark.SparkContext()\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Read Sample Data as Spark DataFrame and assign column names according to RecSys Challenge Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"data/training_sample.tsv\"\n",
    "\n",
    "df = (sql.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"sep\", \"\\x01\")\n",
    "    .load(datafile,  inferSchema=\"true\")\n",
    "    .toDF(\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\"engaging_user_account_creation\", \"engaged_follows_engaging\", \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text_tokens: string, hashtags: string, tweet_id: string, present_media: string, present_links: string, present_domains: string, tweet_type: string, language: string, tweet_timestamp: int, engaged_with_user_id: string, engaged_with_user_follower_count: int, engaged_with_user_following_count: int, engaged_with_user_is_verified: boolean, engaged_with_user_account_creation: int, engaging_user_id: string, engaging_user_follower_count: int, engaging_user_following_count: int, engaging_user_is_verified: boolean, engaging_user_account_creation: int, engaged_follows_engaging: boolean, reply_timestamp: double, retweet_timestamp: double, retweet_with_comment_timestamp: double, like_timestamp: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the boolean variables to interger (0 and 1)\n",
    "from pyspark.sql.functions import col, split, when, array\n",
    "df = df.withColumn(\"engaged_with_user_is_verified\",col(\"engaged_with_user_is_verified\").cast(\"Integer\"))\n",
    "df = df.withColumn(\"engaging_user_is_verified\",col(\"engaging_user_is_verified\").cast(\"Integer\"))\n",
    "df = df.withColumn(\"engaged_follows_engaging\",col(\"engaged_follows_engaging\").cast(\"Integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the string representations of lists\n",
    "## Convert the text tokens to array of ints\n",
    "split_text = pyspark.sql.functions.split(df['text_tokens'], '\\t')\n",
    "df = df.withColumn(\"text_tokens\", split_text)\n",
    "\n",
    "## Convert present media to array of strings\n",
    "split_text = pyspark.sql.functions.split(df['present_media'], '\\t')\n",
    "df = df.withColumn(\"present_media\", when(col('present_media').isNull(), array().cast(\"array<string>\")).otherwise(split_text))\n",
    "\n",
    "## Convert present links to array of strings\n",
    "split_text = pyspark.sql.functions.split(df['present_links'], '\\t')\n",
    "df = df.withColumn(\"present_links\", when(col('present_links').isNull(), array().cast(\"array<string>\")).otherwise(split_text))\n",
    "\n",
    "## Convert hashtags to array of strings\n",
    "split_text = pyspark.sql.functions.split(df['hashtags'], '\\t')\n",
    "df = df.withColumn(\"hashtags\", when(col('hashtags').isNull(), array().cast(\"array<string>\")).otherwise(split_text))\n",
    "\n",
    "## Convert present_domains to array of strings\n",
    "split_text = pyspark.sql.functions.split(df['present_domains'], '\\t')\n",
    "df = df.withColumn(\"present_domains\", when(col('present_domains').isNull(), array().cast(\"array<string>\")).otherwise(split_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o1528.limit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3959105b378f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/spark-2.4.1/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mlimit\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \"\"\"\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o1528.limit"
     ]
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['engaged_with_user_follower_count', \n",
    "                'engaged_with_user_following_count', \n",
    "                'engaged_with_user_account_creation',\n",
    "                'engaging_user_follower_count', \n",
    "                'engaging_user_following_count',\n",
    "                'engaging_user_account_creation',\n",
    "                'tweet_timestamp',\n",
    "               ]\n",
    "\n",
    "\n",
    "categorical_cols = ['tweet_type', 'language', \n",
    "                    'engaged_with_user_is_verified', 'engaging_user_is_verified', 'engaged_follows_engaging']\n",
    "\n",
    "id_cols = ['tweet_id', 'engaged_with_user_id', 'engaging_user_id']\n",
    "\n",
    "response_cols = ['reply_timestamp', \n",
    "                 'retweet_timestamp',\n",
    "                 'retweet_with_comment_timestamp', \n",
    "                 'like_timestamp'\n",
    "                ]\n",
    "\n",
    "tweet_feature_cols = ['text_tokens', 'hashtags', 'present_media', 'present_links', 'present_domains']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer, StringIndexer, FeatureHasher, OneHotEncoderEstimator, CountVectorizer,PCA, VectorAssembler\n",
    "\n",
    "nq = 50 # number of quantiles to use\n",
    "\n",
    "def create_quantilesDiscretizer(input_col: str) -> QuantileDiscretizer:\n",
    "    \"\"\"\n",
    "    Create a Quantile Discretizer for a specified column \n",
    "    Uses as output colum the input + _discretized\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_col: str\n",
    "        Name of the Input Column\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    QuantileDiscretizer\n",
    "    \"\"\"\n",
    "    output_col = input_col + \"_discretized\"\n",
    "    return QuantileDiscretizer(numBuckets=nq,\n",
    "                                  relativeError=0.,\n",
    "                                  handleInvalid='keep',\n",
    "                                  inputCol=input_col,\n",
    "                                  outputCol=output_col)\n",
    "\n",
    "def create_stringIndexer(input_col):\n",
    "    \"\"\"\n",
    "    Create a String Indexer for a specified column \n",
    "    Uses as output colum the input + _indexed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_col: str\n",
    "        Name of the Input Column\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    StringIndexer\n",
    "    \"\"\"\n",
    "    output_col = input_col + \"_indexed\"\n",
    "    return StringIndexer(inputCol=input_col,\n",
    "                         outputCol=output_col,\n",
    "                        handleInvalid='keep',)\n",
    "\n",
    "\n",
    "def create_featureHasher(input_col):\n",
    "    \"\"\"\n",
    "    Create a Feature Hasher for a specified column \n",
    "    Uses as output colum the input + _oneHot (creates oneHotEncodings for strings)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_col: str\n",
    "        Name of the Input Column\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    FeatureHasher\n",
    "    \"\"\"\n",
    "    output_col = input_col + \"_oneHot\"\n",
    "    return FeatureHasher(numFeatures=nq,\n",
    "                         inputCols=[input_col],\n",
    "                         outputCol=output_col)\n",
    "\n",
    "\n",
    "def create_countVectorizer(input_col):\n",
    "    output_col = input_col + \"_vectorized\"\n",
    "    return CountVectorizer(inputCol=input_col,\n",
    "                           outputCol=output_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Numeric Features (5.3.1)\n",
    "quantile_discretizers_numeric = [ create_quantilesDiscretizer(col) for col in numeric_cols ]\n",
    "\n",
    "# Encode Categorical Features (5.3.2)\n",
    "string_indexer_categorical = [ create_stringIndexer(col) for col in categorical_cols]\n",
    "\n",
    "# Encode ID Features (5.3.3)\n",
    "id_feature_hashers = [ create_featureHasher(col) for col in id_cols]\n",
    "\n",
    "# Encode Tweet Features (5.3.4 + 5.3.5)\n",
    "tweet_countVectorizers = [ create_countVectorizer(col) for col in tweet_feature_cols]\n",
    "\n",
    "\n",
    "# One-Hot-Encode Features\n",
    "columns_to_encode = [ col+\"_discretized\" for col in numeric_cols]\n",
    "columns_to_encode.extend([ col+\"_indexed\" for col in categorical_cols])\n",
    "encoded_columns = [ col+\"_oneHot\" for col in numeric_cols]\n",
    "encoded_columns.extend([ col+\"_onHot\" for col in categorical_cols])\n",
    "\n",
    "onHotEncoder = OneHotEncoderEstimator(inputCols=columns_to_encode, \n",
    "                                      outputCols=encoded_columns,dropLast=False,handleInvalid=\"keep\" )\n",
    "\n",
    "\n",
    "\n",
    "# Add Vectors with VectorAssembler\n",
    "encoded_columns.extend([ col+\"_oneHot\" for col in id_cols ])\n",
    "num_cat_id_feature_assambler = VectorAssembler(inputCols=encoded_columns,\n",
    "                                               outputCol=\"non_tweet_features\")\n",
    "\n",
    "tweet_features_encoded = [ col+\"_vectorized\" for col in tweet_feature_cols]\n",
    "tweet_feature_assambler = VectorAssembler(inputCols=tweet_features_encoded,\n",
    "                                               outputCol=\"tweet_features\")\n",
    "\n",
    "# Perform Dimensionality Reduction\n",
    "\n",
    "#non_tweet_pca = PCA(k=16, \n",
    "#          inputCol=\"non_tweet_features\", \n",
    "#          outputCol=\"non_tweet_features_reduced\")\n",
    "\n",
    "#tweet_pca = PCA(k=16, \n",
    "#          inputCol=\"tweet_features\", \n",
    "#          outputCol=\"tweet_features_reduced\")\n",
    "\n",
    "features = ['non_tweet_features', 'tweet_features']\n",
    "feature_assambler = VectorAssembler(inputCols=features,\n",
    "                                               outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# create a list of all transformers\n",
    "stages = list()\n",
    "stages.extend(quantile_discretizers_numeric)\n",
    "stages.extend(string_indexer_categorical)\n",
    "stages.extend(id_feature_hashers)\n",
    "stages.append(onHotEncoder)\n",
    "stages.extend(tweet_countVectorizers)\n",
    "stages.append(num_cat_id_feature_assambler)\n",
    "stages.append(tweet_feature_assambler)\n",
    "#stages.append(non_tweet_pca)\n",
    "#stages.append(tweet_pca)\n",
    "stages.append(feature_assambler)\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform dataframe\n",
    "df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(93549, {8: 1.0, 78: 1.0, 128: 1.0, 174: 1.0, 250: 1.0, 303: 1.0, 349: 1.0, 358: 1.0, 382: 1.0, 422: 1.0, 426: 1.0, 430: 1.0, 437: 1.0, 508: 1.0, 540: 1.0, 584: 1.0, 585: 3.0, 586: 1.0, 587: 1.0, 588: 2.0, 590: 1.0, 591: 1.0, 592: 1.0, 593: 1.0, 596: 1.0, 616: 1.0, 624: 1.0, 639: 1.0, 648: 1.0, 651: 1.0, 655: 1.0, 663: 1.0, 671: 1.0, 680: 1.0, 840: 1.0, 955: 1.0, 975: 1.0, 1490: 1.0, 9749: 1.0, 35703: 1.0, 37881: 2.0, 47236: 1.0, 79119: 1.0}))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('features').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Label Columns \n",
    "\n",
    "Create on column with array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  lit, col\n",
    "\n",
    "label_col = [ col(name) for name in response_cols]\n",
    "\n",
    "def encode_response(x):\n",
    "    return when(col(x).isNull(), float(0)).otherwise(float(1))\n",
    "\n",
    "for column in response_cols:\n",
    "    df = df.withColumn(column, encode_response(column))\n",
    "    \n",
    "\n",
    "df = df.withColumn(\"label\", col(\"like_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.select(\"features\", \"label\").randomSplit([0.8, 0.2], seed=1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1539.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 48.0 failed 1 times, most recent failure: Lost task 9.0 in stage 48.0 (TID 358, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)\n\tat java.lang.StringBuilder.<init>(StringBuilder.java:101)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3465)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3281)\n\tat java.io.ObjectInputStream.readString(ObjectInputStream.java:1960)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1607)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimize(LBFGS.scala:142)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:854)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$train$1.apply(MultilayerPerceptronClassifier.scala:249)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$train$1.apply(MultilayerPerceptronClassifier.scala:205)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:205)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)\n\tat java.lang.StringBuilder.<init>(StringBuilder.java:101)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3465)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3281)\n\tat java.io.ObjectInputStream.readString(ObjectInputStream.java:1960)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1607)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e1e7a25e4468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/spark-2.4.1/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1539.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 48.0 failed 1 times, most recent failure: Lost task 9.0 in stage 48.0 (TID 358, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)\n\tat java.lang.StringBuilder.<init>(StringBuilder.java:101)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3465)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3281)\n\tat java.io.ObjectInputStream.readString(ObjectInputStream.java:1960)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1607)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimize(LBFGS.scala:142)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:854)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$train$1.apply(MultilayerPerceptronClassifier.scala:249)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$train$1.apply(MultilayerPerceptronClassifier.scala:205)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:205)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68)\n\tat java.lang.StringBuilder.<init>(StringBuilder.java:101)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3465)\n\tat java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3281)\n\tat java.io.ObjectInputStream.readString(ObjectInputStream.java:1960)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1607)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2030)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2342)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2266)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2124)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\n"
     ]
    }
   ],
   "source": [
    "# create the trainer and set its parameters\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "layers = [93549, 128, 64, 34, 1]\n",
    "\n",
    "trainer = MultilayerPerceptronClassifier(\n",
    "  layers=layers,\n",
    "  blockSize=128,\n",
    "  seed=1234,\n",
    "  maxIter=100)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "val result = model.transform(test)\n",
    "val predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
